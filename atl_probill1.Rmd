---
title: "anti-trans legislation exploratory"
author: "DAAAAMN OER group"
date: "2023-07-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown
This is a workbook for the exploratory data analysis in R on the pro-LGBTQ bills.

The file used here was a copy and paste into the xlsx from the html version of the bill on the legiscan site


```{r}
#library(readxl)
#bills <- read_excel("4-bill-test.xlsx")
#View(bills)
#template_leg <- read_excel("template legislation.xlsx")
#View(template_leg)
#probills <- read_excel("pro-LGBTQbills-test.xlsx")
#View(probills)
IL_probill <- paste(readLines("IL-HB1596.txt"), collapse="\n")

```

## Clean data

We need some tools for text cleaning
```{r}
#install.packages("stopwords")
library(stringr)
library(stopwords)
```
Have a look at the text imported

```{r}
# we are going to do the first pro-LGBTQ bill - an Illinois bill that passed on June 9 2023 was called "The Given Name Act Protection" which addresses pronoun use in schools & is named the same as the Heritage template legislation
#i = 1
billname = "IL_probill"
ltext<-IL_probill
ltext
```


The syntax for removal is: 'str_replace_all(string, pattern, replacement)'. In the RStudio IDE, you can click on the function word and hit F1, and it brings up the help documentation which has some nice examples.

We will go ahead and make several substitutions and replacements. 

### Removing line breaks
```{r}
# Replace all instances of "\n" (newline) with a space - use escape characters
# You have to add a space otherwise words will run together
ltext <- str_replace_all(ltext, "\\\n", " ")
```

### Completely delete a phrase or line
``` {r}
# "\r" is carriage return
ltext <- str_replace_all(ltext, "\\\r", " ")

# We are going to remove "." and "," next. There are a few reasons for this. I see U.S. will be US and there are some places where the end "." is interfering with removing/matching some words for replacement and removal.

# If we were looking at word/sentence relationships, we may have wanted to keep periods and question marks first to define sentences before breaking up into words. 
ltext <- str_replace_all(ltext, "\\.", "")
ltext <- str_replace_all(ltext, ",", " ")
ltext <- str_replace_all(ltext, "'", "")
ltext <- str_replace_all(ltext, "’", "")
ltext <- str_replace_all(ltext, "‘", "")
ltext <- str_replace_all(ltext, "“", "")
ltext <- str_replace_all(ltext, "”", "")
ltext <- str_replace_all(ltext, '"', " ")
ltext <- str_replace_all(ltext, "-", "")
ltext <- str_replace_all(ltext, '_', " ")
ltext <- str_replace_all(ltext, "\\|", " ")
ltext <- str_replace_all(ltext, ":", " ")
ltext <- str_replace_all(ltext, ";", " ")
ltext <- str_replace_all(ltext, "/", "")
ltext <- str_replace_all(ltext, "\\\\", "") #remove \
#ltext <- str_replace_all(ltext, "\\?", "")

# We also have () and [] to be removed
ltext <- str_replace_all(ltext, "\\(|\\)|\\[|\\]", "")

#lets check progress
#ltext

```

### Delete certain phrases

``` {r}
# This text tends to have a 1. and a. and (a) and (b). We already eliminated parentheses and periods, so now we are going to eliminate all single letter words and numbers.  We are intentionally not removing all numbers. There are several references to k12 or under 18 so we want to maintain these.
ltext <- str_replace_all(ltext, " [a-zA-Z0-9] ", " ")

# We now want to make sure that our words will be recognized as the same when we construct relationships, so we have to converting everything to lowercase.
ltext <- str_to_lower(ltext)
ltext
```

Finally, we clean up redundant spaces
``` {r}
# Replace multiple spaces with one space
# s+ means 1 or more spaces.  \\ are escape characters
ltext <- str_replace_all(ltext, "\\s+", " ")
```

Check to make sure there isn't anything else that needs removing, such as numbers.

``` {r}
# Delete spaces at the start and end of our text
ltext<-trimws(ltext, "both")
```


### Cleaning for an effective text analysis

Are there any words that make sense to eliminate in our analysis?

``` {r}
# Identify stop words
# stopwords gives us a list of such words
# otherwise we could manually define these, or even add to this list
stopwords_regex = paste(stopwords('en'), collapse = '\\b|\\b')
stopwords_regex = paste0('\\b', stopwords_regex, '\\b')
ltext <- str_replace_all(ltext, stopwords_regex, '')
```


### Get a vector of words
We now have one list of words in a 1x1 matrix. We need to convert that to a vector with one word in each slot.

``` {r}
# Remember that strsplit returns a list of lists
ltext_ls<- str_split(ltext, " ")
```

I recommend viewing 'ltext_ls' to get a sense of how the list looks.  

I am saving this as a vector and exporting it to csv for later independent use. You can also structure your word vector data so that each row in "bills" is duplicated and then a word vector row. So if my bill had 4 words, I can have 4 rows for bill1 that have the same descriptor data (e.g. Healthcare) and but under the "words" column, rown 1 has the first word, row 2 has the second word, etc. That is probably the best way to structure the data, but it makes it harder to visualize the data when you have over 400 words like bill 1 actually has.

``` {r}
# Grab just the vector
ltext_vec<- ltext_ls[[1]]

# convert to data frame
ltext_vec<-as.data.frame(ltext_vec)
colnames(ltext_vec) <- c("words")

#write a csv of the cleaned words.  That way you can just do the next section without running the cleaning parts.
write.csv(ltext_vec, file = billname, row.names = FALSE)
```

## Visualizing the word vector output

### Summary of the data
```{r}
library(dplyr)
# Get a sense for the data through a table of word counts

# you will need a read command if you are just picking up from the cleaned text
ltext_df<-read.csv(file = billname)

#how many words are there? This may come up later
n_words <- dim(ltext_vec)[1]

# Use dplyr to automatically count the words that exist. This creates a table (in a dataframe structure) in which you have the list of unique words and an absolute frequency count next to each word.
ltext_counts <- ltext_df %>% count(words)

#relative freq
ltext_counts$relf <- ltext_counts$n / n_words
View(ltext_counts)

# use dim() to tell me how big the matrix is
print(dim(ltext_counts))

```

The first number is rows (141) and the second is columns (3). This means that our word vector contains 141 distinct words & 3 columns - the word, the absolute frequency or count, and the relative frequency or probability of occurance. 

Some methods of visualization, like the base plotting function in R, use this form of the data.


### Base plot in R
```{r}
# This is the base plotting function in R
barplot(height=ltext_counts$n, names=ltext_counts$words)

# Great reference
# https://r-graph-gallery.com/208-basic-barplot.html
```

This plot is illegible. Because it has 226 words. We can look instead at just the 20 most frequently used words.

### Top 20

```{r}
#This will order in descending order
counts_ordered <- ltext_counts %>% arrange(desc(n))
View(counts_ordered)

#This will extract the top 20 for visualization purposes
top20 <-  head(counts_ordered, 20)
top20
```


```{r}
# Same but top 20
barplot(height=top20$n, names=top20$words)

```

This is much better, but you cannot read the words, so we will have the words tilted for better readability
```{r}
# las 2 will make vertical labels
# col will make it a little less boring in color
barplot(height=top20$n, names=top20$words, las = 2, col=rgb(0.2,0.4,0.6,0.6) )

# Great reference for options
# https://r-graph-gallery.com/209-the-options-of-barplot.html
```

### Plot in ggplot2

ggplot2 can look prettier :)
```{r}
library(ggplot2)
# ggplot2 version
# help guide: https://r-graph-gallery.com/218-basic-barplots-with-ggplot2.html
ggplot(top20, aes(x=words, y=n)) + 
  geom_bar(stat = "identity")

```

again the figure would be more easily read if the text was vertical
```{r}
ggplot(top20, aes(x=words, y=n)) + 
  geom_bar(stat = "identity", color=rgb(0.1,0.4,0.5,0.7), fill=rgb(0.1,0.4,0.5,0.7)) +
# Rotate x-axis text to make labels vertical
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  labs(x = NULL)  # or xlab("")

```

Reorder from biggest to smallest
```{r}
# This pulls the words as ordered
ordered.words <- top20 %>%
  arrange(desc(n)) %>%
  pull(words)

# This will help ggplot interpret the words as factors, and in particular, factors with a particular order.
top20$words <- ordered(top20$words, levels = ordered.words)

top20 %>%
ggplot(aes(x=words, y=n)) + 
  geom_bar(stat = "identity", color=rgb(0.1,0.4,0.5,0.7), fill=rgb(0.1,0.4,0.5,0.7)) +
# Rotate x-axis text to make labels vertical
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  labs(x = NULL)  # or xlab("")
```


